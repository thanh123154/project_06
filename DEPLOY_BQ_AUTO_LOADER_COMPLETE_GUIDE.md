# ğŸš€ HÆ¯á»šNG DáºªN HOÃ€N CHá»ˆNH: Deploy & Cháº¡y BigQuery Auto Loader

## ğŸ“‹ OVERVIEW - Tá»•ng quan project

Project nÃ y cÃ³ **3 thÃ nh pháº§n chÃ­nh**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. export_to_gcs.py                                        â”‚
â”‚     â†“ Exports tá»« MongoDB â†’ GCS                              â”‚
â”‚                                                              â”‚
â”‚  2. bq/cloud_functions/bq_auto_loader/                      â”‚
â”‚     â†“ Cloud Function (Gen1) tá»± Ä‘á»™ng load GCS â†’ BigQuery    â”‚
â”‚                                                              â”‚
â”‚  3. bq/scripts/load_jsonl_from_gcs.py                       â”‚
â”‚     â†“ Script manual load (backup)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow hoÃ n chá»‰nh:

```
MongoDB â†’ export_to_gcs.py â†’ GCS â†’ bq_auto_loader â†’ BigQuery
```

---

## ğŸ” PHáº¦N 1: Cáº¤U TRÃšC PROJECT

### Files vÃ  thÆ° má»¥c:

```
project_06/
â”œâ”€â”€ export_to_gcs.py                    # Script export MongoDB â†’ GCS
â”œâ”€â”€ exporter/                           # Module export
â”‚   â”œâ”€â”€ mongo_exporter.py              # MongoDB connection & export
â”‚   â”œâ”€â”€ gcs.py                         # GCS upload
â”‚   â”œâ”€â”€ writers.py                     # File writers (JSONL, CSV, etc.)
â”‚   â””â”€â”€ utils.py                       # Utilities
â”‚
â”œâ”€â”€ schema/
â”‚   â””â”€â”€ glamira_schema_raw.json        # BigQuery schema
â”‚
â”œâ”€â”€ bq/
â”‚   â”œâ”€â”€ cloud_functions/
â”‚   â”‚   â”œâ”€â”€ bq_auto_loader/           # âœ… Cloud Function chÃ­nh
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py               # Function code
â”‚   â”‚   â”‚   â”œâ”€â”€ deploy.sh             # Deploy script
â”‚   â”‚   â”‚   â”œâ”€â”€ requirements.txt       # Dependencies
â”‚   â”‚   â”‚   â””â”€â”€ schema.json           # Schema (copy tá»« root)
â”‚   â”‚   â””â”€â”€ bq_loader/                # Alternative (khÃ´ng dÃ¹ng)
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ load_jsonl_from_gcs.py    # Manual load script
â”‚
â””â”€â”€ requirements.txt                    # Main dependencies
```

---

## ğŸ› ï¸ PHáº¦N 2: SETUP BAN Äáº¦U

### BÆ°á»›c 1: Install dependencies

```bash
# KÃ­ch hoáº¡t venv (náº¿u cÃ³)
.\venv\Scripts\activate

# Install packages cho export script
pip install -r requirements.txt

# Install packages cho Cloud Function
pip install -r bq/cloud_functions/bq_auto_loader/requirements.txt
```

### BÆ°á»›c 2: Setup Google Cloud

```bash
# Login
gcloud auth login

# Set project
gcloud config set project consummate-rig-466909-i6

# Báº­t cÃ¡c APIs cáº§n thiáº¿t
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
```

### BÆ°á»›c 3: Setup Service Account

```bash
# Set credentials cho export script
$env:GOOGLE_APPLICATION_CREDENTIALS="C:\path\to\service-account-key.json"
```

**Hoáº·c thÃªm vÃ o .env:**

```env
GOOGLE_APPLICATION_CREDENTIALS=C:\path\to\service-account-key.json
```

---

## ğŸš€ PHáº¦N 3: DEPLOY CLOUD FUNCTION

### âš™ï¸ ThÃ´ng tin quan trá»ng:

| Item              | Value                                     |
| ----------------- | ----------------------------------------- |
| **Function Name** | `bq-auto-loader`                          |
| **Runtime**       | Python 3.11                               |
| **Trigger**       | GCS Object Finalize                       |
| **Bucket**        | `first-bucket-practice-for-data-engineer` |
| **Path Pattern**  | `exports/daily/*.jsonl`                   |
| **Region**        | `us-central1`                             |
| **Memory**        | 1GB                                       |
| **Timeout**       | 1800s (30 min)                            |
| **Project**       | `consummate-rig-466909-i6`                |
| **Dataset**       | `my_raw_dataset`                          |
| **Table**         | `events2`                                 |

---

### CÃCH 1: Deploy báº±ng script (Khuyáº¿n nghá»‹)

#### TrÃªn Linux/Mac:

```bash
cd bq/cloud_functions/bq_auto_loader

# Cáº¥p quyá»n execute
chmod +x deploy.sh

# Cháº¡y deploy
./deploy.sh
```

#### Script sáº½ lÃ m gÃ¬:

```bash
# 1. Copy schema tá»« root
cp ../../../schema/glamira_schema_raw.json ./schema.json

# 2. Deploy Cloud Function
gcloud functions deploy bq-auto-loader \
  --runtime python311 \
  --trigger-event-type google.storage.object.finalize \
  --trigger-resource first-bucket-practice-for-data-engineer \
  --trigger-event-filters path-pattern="exports/daily/*.jsonl" \
  --set-env-vars PROJECT_ID=consummate-rig-466909-i6 \
  --set-env-vars DATASET=my_raw_dataset \
  --set-env-vars TABLE=events2 \
  --set-env-vars WRITE_DISPOSITION=WRITE_APPEND \
  --set-env-vars MAX_BAD_RECORDS=1000 \
  --set-env-vars SCHEMA_PATH=/workspace/schema.json \
  --memory=1GB \
  --timeout=1800s \
  --max-instances=10 \
  --region=us-central1

# 3. Test vá»›i file máº«u
echo '{"test": "data"}' | gsutil cp - "gs://bucket/exports/daily/test.jsonl"
```

---

### CÃCH 2: Deploy thá»§ cÃ´ng trÃªn Windows

#### BÆ°á»›c 1: Copy schema

```powershell
# Tá»« project root
Copy-Item schema\glamira_schema_raw.json bq\cloud_functions\bq_auto_loader\schema.json
```

#### BÆ°á»›c 2: Deploy function

```powershell
cd bq\cloud_functions\bq_auto_loader

gcloud functions deploy bq-auto-loader `
  --runtime python311 `
  --trigger-event-type google.storage.object.finalize `
  --trigger-resource first-bucket-practice-for-data-engineer `
  --trigger-event-filters path-pattern="exports/daily/*.jsonl" `
  --set-env-vars PROJECT_ID=consummate-rig-466909-i6 `
  --set-env-vars DATASET=my_raw_dataset `
  --set-env-vars TABLE=events2 `
  --set-env-vars WRITE_DISPOSITION=WRITE_APPEND `
  --set-env-vars MAX_BAD_RECORDS=1000 `
  --set-env-vars SCHEMA_PATH=/workspace/schema.json `
  --memory=1GB `
  --timeout=1800s `
  --max-instances=10 `
  --region=us-central1
```

---

## ğŸ§ª PHáº¦N 4: TEST FUNCTION

### Test 1: Upload file Ä‘á»ƒ trigger

```powershell
# Upload file lÃªn GCS
gsutil cp test_file.jsonl gs://first-bucket-practice-for-data-engineer/exports/daily/test.jsonl
```

### Test 2: Xem logs

```powershell
# Xem logs gáº§n nháº¥t
gcloud functions logs read bq-auto-loader --region=us-central1 --limit=20

# Xem logs real-time
gcloud functions logs read bq-auto-loader --region=us-central1 --follow

# Xem logs errors only
gcloud functions logs read bq-auto-loader --region=us-central1 --severity=ERROR --limit=50
```

### Test 3: Kiá»ƒm tra BigQuery

```powershell
# Xem table
bq show consummate-rig-466909-i6:my_raw_dataset.events2

# Query sá»‘ lÆ°á»£ng records
bq query --use_legacy_sql=false "SELECT COUNT(*) FROM \`consummate-rig-466909-i6.my_raw_dataset.events2\`"

# Xem 10 rows má»›i nháº¥t
bq query --use_legacy_sql=false "SELECT * FROM \`consummate-rig-466909-i6.my_raw_dataset.events2\` ORDER BY time_stamp DESC LIMIT 10"
```

---

## ğŸ¯ PHáº¦N 5: WORKFLOW HOÃ€N CHá»ˆNH

### End-to-end pipeline:

```bash
# BÆ¯á»šC 1: Export tá»« MongoDB â†’ GCS
python export_to_gcs.py `
  --db your_database `
  --collection your_collection `
  --query-file sample_query.json `
  --format jsonl `
  --batch-size 5000 `
  --gcs-bucket first-bucket-practice-for-data-engineer `
  --gcs-prefix exports/daily

# Output: gs://first-bucket-practice-for-data-engineer/exports/daily/export_20240124_143052.jsonl

# BÆ¯á»šC 2: Cloud Function tá»± Ä‘á»™ng trigger khi file upload xong

# BÆ¯á»šC 3: Kiá»ƒm tra logs
gcloud functions logs read bq-auto-loader --region=us-central1 --limit=20

# BÆ¯á»šC 4: Verify data trong BigQuery
bq query --use_legacy_sql=false "SELECT COUNT(*) FROM \`consummate-rig-466909-i6.my_raw_dataset.events2\`"
```

---

## ğŸ“Š PHáº¦N 6: MONITORING & DEBUGGING

### Xem function status

```powershell
# Chi tiáº¿t function
gcloud functions describe bq-auto-loader --region=us-central1

# List táº¥t cáº£ functions
gcloud functions list --region=us-central1
```

### Xem BigQuery jobs

```powershell
# List jobs
bq ls -j --max_results=10 consummate-rig-466909-i6

# Xem job detail
bq show -j JOB_ID

# Cancel job
bq cancel -j JOB_ID
```

### Web Console

```
https://console.cloud.google.com/functions/details/us-central1/bq-auto-loader?project=consummate-rig-466909-i6
```

---

## ğŸ”§ PHáº¦N 7: TROUBLESHOOTING

### 1. Function khÃ´ng trigger

**Kiá»ƒm tra:**

```powershell
# File pattern Ä‘Ãºng chÆ°a?
# Pháº£i lÃ : exports/daily/*.jsonl
# Pháº£i cÃ¹ng bucket: first-bucket-practice-for-data-engineer

# Upload láº¡i Ä‘á»ƒ test
gsutil cp test.jsonl gs://first-bucket-practice-for-data-engineer/exports/daily/test.jsonl
```

### 2. Permission errors

```powershell
# Kiá»ƒm tra service account
gcloud functions describe bq-auto-loader --region=us-central1 --gen2

# Cáº¥p thÃªm quyá»n
PROJECT_ID="consummate-rig-466909-i6"
SERVICE_ACCOUNT="bq-auto-loader@${PROJECT_ID}.iam.gserviceaccount.com"

gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/bigquery.admin"

gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/storage.objectViewer"
```

### 3. Timeout errors

**Giáº£i phÃ¡p:**

```powershell
# TÄƒng timeout vÃ  memory
gcloud functions deploy bq-auto-loader `
  --timeout=1800s `
  --memory=2GB `
  --region=us-central1
```

### 4. Schema errors

```powershell
# Kiá»ƒm tra schema
cat bq\cloud_functions\bq_auto_loader\schema.json

# Validate schema
bq show --schema --format=prettyjson consummate-rig-466909-i6:my_raw_dataset.events2
```

---

## ğŸ“ PHáº¦N 8: UPDATE FUNCTION

### Update code

```powershell
cd bq\cloud_functions\bq_auto_loader

# Chá»‰ update code
gcloud functions deploy bq-auto-loader --source . --region=us-central1
```

### Update environment variables

```powershell
gcloud functions deploy bq-auto-loader `
  --update-env-vars TABLE=new_table,MAX_BAD_RECORDS=2000 `
  --region=us-central1
```

### Delete function

```powershell
gcloud functions delete bq-auto-loader --region=us-central1
```

---

## âš™ï¸ PHáº¦N 9: Cáº¤U HÃŒNH NÃ‚NG CAO

### Environment Variables:

| Variable                | Description                                     | Default                    |
| ----------------------- | ----------------------------------------------- | -------------------------- |
| `PROJECT_ID`            | GCP Project ID                                  | `consummate-rig-466909-i6` |
| `DATASET`               | BigQuery dataset                                | `my_raw_dataset`           |
| `TABLE`                 | BigQuery table                                  | `events2`                  |
| `WRITE_DISPOSITION`     | `WRITE_APPEND`, `WRITE_TRUNCATE`, `WRITE_EMPTY` | `WRITE_APPEND`             |
| `MAX_BAD_RECORDS`       | Sá»‘ records bad tá»‘i Ä‘a                           | `1000`                     |
| `IGNORE_UNKNOWN_VALUES` | Ignore fields khÃ´ng cÃ³ trong schema             | `true`                     |
| `SCHEMA_PATH`           | ÄÆ°á»ng dáº«n schema                                | `/workspace/schema.json`   |

### File Patterns:

- âœ… **Extensions:** `.jsonl`, `.json`
- âœ… **Path:** `exports/daily/*.jsonl`
- âŒ **Excludes:** `_fixed`, `_chunk_`, `_temp`

---

## ğŸ“š PHáº¦N 10: TÃ€I LIá»†U THAM KHáº¢O

### Useful Commands:

```powershell
# Export tá»« MongoDB
python export_to_gcs.py --db db --collection collection --query-file query.json --format jsonl --gcs-bucket bucket

# Deploy Cloud Function
cd bq\cloud_functions\bq_auto_loader
.\deploy.sh  # hoáº·c deploy thá»§ cÃ´ng

# Upload test file
gsutil cp test.jsonl gs://bucket/exports/daily/test.jsonl

# Xem logs
gcloud functions logs read bq-auto-loader --region=us-central1 --limit=20

# Kiá»ƒm tra BigQuery
bq query "SELECT COUNT(*) FROM \`project.dataset.table\`"
```

### Links:

- Cloud Functions Docs: https://cloud.google.com/functions/docs
- BigQuery Docs: https://cloud.google.com/bigquery/docs
- GCS Triggers: https://cloud.google.com/functions/docs/tutorials/storage

---

## âœ… CHECKLIST DEPLOY

- [ ] Setup Google Cloud authentication
- [ ] Báº­t cÃ¡c APIs cáº§n thiáº¿t
- [ ] Install dependencies
- [ ] Copy schema file vÃ o bq_auto_loader
- [ ] Deploy Cloud Function
- [ ] Test vá»›i file upload
- [ ] Check logs
- [ ] Verify data trong BigQuery

---

## ğŸ‰ DONE!

Sau khi hoÃ n thÃ nh táº¥t cáº£ cÃ¡c bÆ°á»›c trÃªn, báº¡n sáº½ cÃ³:

1. âœ… Cloud Function `bq-auto-loader` Ä‘Ã£ deploy
2. âœ… Tá»± Ä‘á»™ng load file tá»« GCS â†’ BigQuery
3. âœ… Monitoring vÃ  logging Ä‘áº§y Ä‘á»§
4. âœ… Pipeline hoÃ n chá»‰nh: MongoDB â†’ GCS â†’ BigQuery
